name: Test Suite

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  UV_CACHE_DIR: ~/.cache/uv

jobs:
  # Fast smoke tests for quick feedback
  smoke-tests:
    name: Smoke Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Cache UV dependencies
      uses: actions/cache@v4
      with:
        path: ${{ env.UV_CACHE_DIR }}
        key: uv-${{ runner.os }}-${{ hashFiles('**/uv.lock') }}
        restore-keys: |
          uv-${{ runner.os }}-
    
    - name: Install dependencies
      run: |
        cd configs
        uv pip install -e ".[dev]"
    
    - name: Run smoke tests
      run: |
        python scripts/ci_test_runner.py --category smoke
    
    - name: Upload smoke test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: smoke-test-results
        path: artifacts/
        retention-days: 7

  # Comprehensive test suite
  full-tests:
    name: Full Test Suite
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    needs: smoke-tests
    
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest, windows-latest, macos-latest ]
        python-version: [ '3.9', '3.10', '3.11', '3.12' ]
        exclude:
          # Reduce matrix size for efficiency
          - os: windows-latest
            python-version: '3.9'
          - os: macos-latest
            python-version: '3.9'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
      shell: bash
    
    - name: Cache UV dependencies
      uses: actions/cache@v4
      with:
        path: ${{ env.UV_CACHE_DIR }}
        key: uv-${{ matrix.os }}-${{ matrix.python-version }}-${{ hashFiles('**/uv.lock') }}
        restore-keys: |
          uv-${{ matrix.os }}-${{ matrix.python-version }}-
          uv-${{ matrix.os }}-
    
    - name: Install dependencies
      run: |
        cd configs
        uv pip install -e ".[dev]"
      shell: bash
    
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y libmagic1 poppler-utils
    
    - name: Run linting
      run: |
        cd configs
        ruff check ../backend ../tests --output-format=github
        ruff format ../backend ../tests --check
        mypy ../backend
      shell: bash
    
    - name: Run full test suite
      run: |
        python scripts/ci_test_runner.py --category full
      shell: bash
    
    - name: Generate test report
      if: always()
      run: |
        python scripts/test_report_generator.py
        python scripts/test_metrics_collector.py
      shell: bash
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          artifacts/
          !artifacts/.pytest_cache
        retention-days: 30
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v5
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      with:
        file: artifacts/coverage.xml
        flags: backend
        name: backend-coverage
        fail_ci_if_error: false
    
    - name: Comment PR with test results
      uses: actions/github-script@v7
      if: github.event_name == 'pull_request' && matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      with:
        script: |
          const fs = require('fs');
          const path = 'artifacts/ci_summary.json';
          
          if (fs.existsSync(path)) {
            const summary = JSON.parse(fs.readFileSync(path, 'utf8'));
            const success = summary.success;
            const icon = success ? '✅' : '❌';
            const status = success ? 'PASSED' : 'FAILED';
            
            const body = `## ${icon} Test Results ${status}
            
            **Environment:** ${{ matrix.os }} - Python ${{ matrix.python-version }}
            **Execution Time:** ${summary.test_results.total_time.toFixed(1)}s
            **Retries:** ${summary.test_results.retry_count}
            
            View detailed results in the [artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}).
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
          }

  # Frontend tests
  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run frontend linting
      run: |
        cd frontend
        npm run lint
    
    - name: Run frontend type checking
      run: |
        cd frontend
        npm run type-check
    
    - name: Run frontend tests
      run: |
        cd frontend
        npm test -- --coverage --watchAll=false
    
    - name: Upload frontend coverage
      uses: codecov/codecov-action@v5
      with:
        directory: frontend/coverage
        flags: frontend
        name: frontend-coverage
        fail_ci_if_error: false

  # E2E tests with Playwright
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [smoke-tests, frontend-tests]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Install backend dependencies
      run: |
        cd configs
        uv pip install -e ".[dev]"
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Install Playwright browsers
      run: |
        cd frontend
        npx playwright install --with-deps
    
    - name: Build frontend
      run: |
        cd frontend
        npm run build
    
    - name: Start backend server
      run: |
        cd backend
        uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10  # Wait for server to start
    
    - name: Run E2E tests
      run: |
        cd frontend
        npm run test:e2e
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-results
        path: |
          frontend/test-results/
          frontend/playwright-report/
        retention-days: 7

  # Performance benchmarks
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event_name == 'push' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Install dependencies
      run: |
        cd configs
        uv pip install -e ".[dev]"
    
    - name: Run performance tests
      run: |
        python -m pytest tests/ -m performance --benchmark-only --benchmark-json=artifacts/benchmark.json
    
    - name: Upload performance results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: artifacts/benchmark.json
        retention-days: 30

  # Security scanning
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Install dependencies
      run: |
        cd configs
        uv pip install -e ".[dev]"
        uv pip install safety bandit
    
    - name: Run safety check
      run: |
        cd configs
        uv pip freeze | safety check --json --output artifacts/safety-report.json || true
    
    - name: Run bandit security scan
      run: |
        bandit -r backend/ -f json -o artifacts/bandit-report.json || true
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-scan-results
        path: artifacts/*-report.json
        retention-days: 30

  # Test summary and badges
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [full-tests, frontend-tests, e2e-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v4
    
    - name: Generate test summary
      run: |
        python3 -c "
        import json
        import glob
        import os
        
        # Collect all test results
        summary = {
            'total_jobs': 0,
            'successful_jobs': 0,
            'failed_jobs': 0,
            'backend_coverage': 0,
            'frontend_coverage': 0
        }
        
        # Check job results from needs context
        needs_results = '${{ toJSON(needs) }}'
        if needs_results != 'null':
            needs = json.loads(needs_results)
            for job_name, job_result in needs.items():
                summary['total_jobs'] += 1
                if job_result['result'] == 'success':
                    summary['successful_jobs'] += 1
                else:
                    summary['failed_jobs'] += 1
        
        # Generate badge data
        success_rate = (summary['successful_jobs'] / summary['total_jobs'] * 100) if summary['total_jobs'] > 0 else 0
        
        badge_data = {
            'schemaVersion': 1,
            'label': 'tests',
            'message': f\"{summary['successful_jobs']}/{summary['total_jobs']} passed\",
            'color': 'green' if success_rate >= 90 else 'orange' if success_rate >= 70 else 'red'
        }
        
        with open('test-badge.json', 'w') as f:
            json.dump(badge_data, f, indent=2)
        
        with open('test-summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f'Test Summary: {summary[\"successful_jobs\"]}/{summary[\"total_jobs\"]} jobs passed')
        "
    
    - name: Upload test summary
      uses: actions/upload-artifact@v4
      with:
        name: test-summary
        path: |
          test-badge.json
          test-summary.json
        retention-days: 90
    
    - name: Update badge (if main branch)
      if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
      run: |
        # This would typically push to a badges branch or external service
        echo "Would update test badge with results"